{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy import stats\n",
    "from pathlib import Path, PureWindowsPath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataset_info(data_path):\n",
    "    # extract information from train.txt\n",
    "    f = open(os.path.join(data_path, \"train.txt\"), \"r\")\n",
    "    contents_train = f.readlines()\n",
    "    label_classes, label_train_list, img_train_list = [], [], []\n",
    "    for sample in contents_train:\n",
    "        sample = sample.split()\n",
    "        label, img_path = sample[0], sample[1]\n",
    "        if label not in label_classes:\n",
    "            label_classes.append(label)\n",
    "        label_train_list.append(sample[0])\n",
    "        img_train_list.append(os.path.join(data_path, Path(PureWindowsPath(img_path))))\n",
    "    print('Classes: {}'.format(label_classes))\n",
    "\n",
    "    # extract information from test.txt\n",
    "    f = open(os.path.join(data_path, \"test.txt\"), \"r\")\n",
    "    contents_test = f.readlines()\n",
    "    label_test_list, img_test_list = [], []\n",
    "    for sample in contents_test:\n",
    "        sample = sample.split()\n",
    "        label, img_path = sample[0], sample[1]\n",
    "        label_test_list.append(label)\n",
    "        img_test_list.append(os.path.join(data_path, Path(PureWindowsPath(img_path))))  # you can directly use img_path if you run in Windows\n",
    "\n",
    "    return label_classes, label_train_list, img_train_list, label_test_list, img_test_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dsift(img, stride, size):\n",
    "    # To do\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    step_size = 20\n",
    "    \n",
    "    ## 不确定 KeyPoint(x, y, size)\n",
    "    kp = [cv2.KeyPoint(x+size/2, y+size/2, size) for y in range(0, img.shape[0], stride) \n",
    "                                    for x in range(0, img.shape[1], stride)]\n",
    "\n",
    "    kp1, dense_feature = sift.compute(img , kp)\n",
    "    return dense_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiny_image(img, output_size):\n",
    "    # To do\n",
    "    img_resize = cv2.resize(img, output_size, interpolation = cv2.INTER_AREA)\n",
    "    #normalization\n",
    "    \n",
    "    feature = np.zeros(output_size)\n",
    "    \n",
    "    feature_omean= img_resize- np.mean(img_resize)\n",
    "       \n",
    "    feature = feature_omean / np.linalg.norm(feature_omean.reshape((1,np.prod(feature_omean.shape))))\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_knn(feature_train, label_train, feature_test, k):\n",
    "    # To do\n",
    "    nbrs = NearestNeighbors(algorithm='auto').fit(feature_train)\n",
    "    \n",
    "    distances_map,indices_map =nbrs.kneighbors(feature_test,n_neighbors=k)\n",
    "    \n",
    "    label_test_pred = np.zeros(len(indices_map))\n",
    "    \n",
    "    for num in range(len(indices_map)):\n",
    "        label_test_pred[num] = np.argmax(np.bincount(label_train[indices_map[num,:]]))\n",
    "    \n",
    "    return label_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_knn_tiny(label_classes, label_train_list, img_train_list, label_test_list, img_test_list):\n",
    "    # To do\n",
    "    #1\n",
    "    feature_vec = []\n",
    "    output_size = output_size= (16,16)\n",
    "    k = 10\n",
    "    for name in img_train_list :\n",
    "        img = cv2.imread(name,0)\n",
    "        feature = get_tiny_image(img, output_size)\n",
    "        feature_vec.append( feature.reshape(np.prod(feature.shape)))\n",
    "    #2\n",
    "    feature_test_vec = []\n",
    "    for name in img_test_list :\n",
    "        img = cv2.imread(name,0)\n",
    "        feature_test = get_tiny_image(img, output_size)\n",
    "        feature_test_vec.append( feature_test.reshape(np.prod(feature_test.shape)))  \n",
    "    #3  \n",
    "    label_train_set = []\n",
    "    for item in label_train_list:\n",
    "        label_train_set.append(label_classes.index(item))\n",
    "    \n",
    "    label_train_set = np.array(label_train_set)\n",
    "    #4\n",
    "    label_test_set = []\n",
    "    for item in label_test_list:\n",
    "        label_test_set.append(label_classes.index(item))\n",
    "    \n",
    "    label_test_set = np.array(label_test_set)\n",
    "    \n",
    "    \n",
    "    #predict\n",
    "    label_test_pred = predict_knn(feature_vec, label_train_set, feature_test_vec, k)\n",
    "    \n",
    "    #confusion matrix]\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    y_true = label_test_set\n",
    "    y_pred = label_test_pred\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "    accuracy = np.trace(confusion)/np.sum(np.sum(confusion))\n",
    "    \n",
    "    \n",
    "    visualize_confusion_matrix(confusion, accuracy, label_classes)\n",
    "    return confusion, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_visual_dictionary(dense_feature_list, dic_size=50):\n",
    "    # To do\n",
    "    dense_feature=np.zeros((1,128))\n",
    "    \n",
    "    for item in dense_feature_list:\n",
    "\n",
    "        dense_feature = np.concatenate((dense_feature, item), axis=0)\n",
    "    \n",
    "    dense_feature_set = np.delete(dense_feature, 0, 0)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters = 50,n_init=10,max_iter=300).fit(dense_feature_set)\n",
    "    vocab = kmeans.cluster_centers_\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bow(feature, vocab):\n",
    "    # To do\n",
    "    nbrs = NearestNeighbors(algorithm='auto').fit(vocab) \n",
    "    \n",
    "    distances_map,indices_map =nbrs.kneighbors(feature_test,n_neighbors=1)\n",
    "    \n",
    "    bow_feature_pre = indices_map.reshape((1,len(vocab)))\n",
    "    \n",
    "    bow_feature = np.bincount(bow_feature_pre,minlength=len(vocab))/len(vocab)\n",
    "    \n",
    "    return bow_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_knn_bow(label_classes, label_train_list, img_train_list, label_test_list, img_test_list):\n",
    "    # To do\n",
    "    stride, size = (20,20)\n",
    "    \n",
    "    dense_feature_list=[]\n",
    "    for name in img_train_list :\n",
    "        img = cv2.imread(name,0)\n",
    "        stride, size = (20,20)\n",
    "        dense_feature = compute_dsift(img, stride, size)\n",
    "        dense_feature_list.append(dense_feature)\n",
    "\n",
    "    vocab = build_visual_dictionary(dense_feature_list, 50)\n",
    "    np.savetxt('test_knn_bow_out', vocab, delimiter=',')\n",
    "    \n",
    "    vocab_feature_list=[]\n",
    "    for name in img_train_list :\n",
    "        img = cv2.imread(name,0)\n",
    "        stride, size = (20,20)\n",
    "        dense_feature = compute_dsift(img, stride, size)\n",
    "        \n",
    "        bow_feature = compute_bow(dense_feature, vocab)\n",
    "        \n",
    "        vocab_feature_list.append(bow_feature)\n",
    "    \n",
    "    #2\n",
    "    vocab_feature_test_list = []\n",
    "    for name in img_test_list :\n",
    "        img = cv2.imread(name,0)\n",
    "        dense_feature_test = compute_dsift(img, stride, size)\n",
    "        \n",
    "        bow_feature_test = compute_bow(dense_feature_test, vocab)\n",
    "        \n",
    "        vocab_feature_test_list.append(dense_feature_test)\n",
    "        \n",
    "    #3  \n",
    "    label_train_set = []\n",
    "    for item in label_train_list:\n",
    "        label_train_set.append(label_classes.index(item))\n",
    "    \n",
    "    label_train_set = np.array(label_train_set)\n",
    "    #4\n",
    "    label_test_set = []\n",
    "    for item in label_test_list:\n",
    "        label_test_set.append(label_classes.index(item))\n",
    "    \n",
    "    label_test_set = np.array(label_test_set)\n",
    "    \n",
    "    k = 10\n",
    "    \n",
    "    label_test_pred = predict_knn(vocab_feature_list, label_train_set, vocab_feature_test_list, k)\n",
    "    \n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    y_true = label_test_set\n",
    "    y_pred = label_test_pred\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "    accuracy = np.trace(confusion)/np.sum(np.sum(confusion))\n",
    "    \n",
    "    \n",
    "    visualize_confusion_matrix(confusion, accuracy, label_classes)\n",
    "\n",
    "    return confusion, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_svm(feature_train, label_train, feature_test, n_classes):\n",
    "    # To do\n",
    "    clf = LinearSVC(tol=1e-5,C=1.0) \n",
    "    clf.fit(feature_train, label_train)  \n",
    "    \n",
    "    label_test_pred = clf.predict(feature_test)\n",
    "    \n",
    "    \n",
    "    return label_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_svm_bow(label_classes, label_train_list, img_train_list, label_test_list, img_test_list):\n",
    "    # To do\n",
    "    \n",
    "    dense_feature_list=[]\n",
    "    for name in img_train_list :\n",
    "        img = cv2.imread(name,0)\n",
    "        stride, size = (20,20)\n",
    "        dense_feature = compute_dsift(img, stride, size)\n",
    "        dense_feature_list.append(dense_feature)\n",
    "\n",
    "    vocab = build_visual_dictionary(dense_feature_list, 50)\n",
    "    np.savetxt('test_svm_bow_out', vocab, delimiter=',')\n",
    "    \n",
    "    vocab_feature_list=[]\n",
    "    for name in img_train_list :\n",
    "        img = cv2.imread(name,0)\n",
    "        stride, size = (20,20)\n",
    "        dense_feature = compute_dsift(img, stride, size)\n",
    "        \n",
    "        bow_feature = compute_bow(dense_feature, vocab)\n",
    "        \n",
    "        vocab_feature_list.append(bow_feature)\n",
    "    \n",
    "    #2\n",
    "    vocab_feature_test_list = []\n",
    "    for name in img_test_list :\n",
    "        img = cv2.imread(name,0)\n",
    "        dense_feature_test = compute_dsift(img, stride, size)\n",
    "        \n",
    "        bow_feature_test = compute_bow(dense_feature_test, vocab)\n",
    "        \n",
    "        vocab_feature_test_list.append(dense_feature_test)\n",
    "        \n",
    "    #3  \n",
    "    label_train_set = []\n",
    "    for item in label_train_list:\n",
    "        label_train_set.append(label_classes.index(item))\n",
    "    \n",
    "    label_train_set = np.array(label_train_set)\n",
    "    #4\n",
    "    label_test_set = []\n",
    "    for item in label_test_list:\n",
    "        label_test_set.append(label_classes.index(item))\n",
    "    \n",
    "    label_test_set = np.array(label_test_set)\n",
    "    \n",
    "    label_test_pred = predict_svm(feature_train, label_train, feature_test, n_classes)\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    y_true = label_test_set\n",
    "    y_pred = label_test_pred\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "    accuracy = np.trace(confusion)/np.sum(np.sum(confusion))\n",
    "    \n",
    "    visualize_confusion_matrix(confusion, accuracy, label_classes)\n",
    "    return confusion, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_confusion_matrix(confusion, accuracy, label_classes):\n",
    "    plt.title(\"accuracy = {:.3f}\".format(accuracy))\n",
    "    plt.imshow(confusion)\n",
    "    ax, fig = plt.gca(), plt.gcf()\n",
    "    plt.xticks(np.arange(len(label_classes)), label_classes)\n",
    "    plt.yticks(np.arange(len(label_classes)), label_classes)\n",
    "    # set horizontal alignment mode (left, right or center) and rotation mode(anchor or default)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"center\", rotation_mode=\"default\")\n",
    "    # avoid top and bottom part of heatmap been cut\n",
    "    ax.set_xticks(np.arange(len(label_classes) + 1) - .5, minor=True)\n",
    "    ax.set_yticks(np.arange(len(label_classes) + 1) - .5, minor=True)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # To do: replace with your dataset path\n",
    "    label_classes, label_train_list, img_train_list, label_test_list, img_test_list = extract_dataset_info(\"./scene_classification_data\")\n",
    "    \n",
    "    classify_knn_tiny(label_classes, label_train_list, img_train_list, label_test_list, img_test_list)\n",
    "\n",
    "    classify_knn_bow(label_classes, label_train_list, img_train_list, label_test_list, img_test_list)\n",
    "    \n",
    "    classify_svm_bow(label_classes, label_train_list, img_train_list, label_test_list, img_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
