{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy import stats\n",
    "from pathlib import Path, PureWindowsPath\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Confusion_M(y_t,y_p):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_dataset_info(data_path):\n",
    "    # extract information from train.txt\n",
    "    f = open(os.path.join(data_path, \"train.txt\"), \"r\")\n",
    "    contents_train = f.readlines()\n",
    "    label_classes, label_train_list, img_train_list = [], [], []\n",
    "    for sample in contents_train:\n",
    "        sample = sample.split()\n",
    "        label, img_path = sample[0], sample[1]\n",
    "        if label not in label_classes:\n",
    "            label_classes.append(label)\n",
    "        label_train_list.append(sample[0])\n",
    "        img_train_list.append(os.path.join(data_path, Path(PureWindowsPath(img_path))))\n",
    "    print('Classes: {}'.format(label_classes))\n",
    "\n",
    "    # extract information from test.txt\n",
    "    f = open(os.path.join(data_path, \"test.txt\"), \"r\")\n",
    "    contents_test = f.readlines()\n",
    "    label_test_list, img_test_list = [], []\n",
    "    for sample in contents_test:\n",
    "        sample = sample.split()\n",
    "        label, img_path = sample[0], sample[1]\n",
    "        label_test_list.append(label)\n",
    "        img_test_list.append(os.path.join(data_path, Path(PureWindowsPath(img_path))))  # you can directly use img_path if you run in Windows\n",
    "\n",
    "    return label_classes, label_train_list, img_train_list, label_test_list, img_test_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dsift(img, stride, size):\n",
    "    # To do\n",
    "    sift = cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "    step_size = 16\n",
    "    \n",
    "    ## 不确定 KeyPoint(x, y, size)\n",
    "    kp = [cv2.KeyPoint(x+size/2, y+size/2, size) for y in range(0, img.shape[0], stride) \n",
    "                                    for x in range(0, img.shape[1], stride)]\n",
    "\n",
    "    kp1, dense_feature = sift.compute(img , kp)\n",
    "    return dense_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tiny_image(img, output_size):\n",
    "    # To do\n",
    "    img_resize = cv2.resize(img, output_size, interpolation = cv2.INTER_AREA)\n",
    "    #normalization\n",
    "    \n",
    "    feature = np.zeros(output_size)\n",
    "    \n",
    "    feature_omean= img_resize- np.mean(img_resize)\n",
    "       \n",
    "    feature = feature_omean / np.linalg.norm(feature_omean.reshape\n",
    "                                             ((1,np.prod(feature_omean.shape))))\n",
    "    \n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_knn(feature_train, label_train, feature_test, k):\n",
    "    # To do\n",
    "    nbrs = NearestNeighbors(algorithm='auto').fit(feature_train)\n",
    "    \n",
    "    distances_map,indices_map =nbrs.kneighbors(feature_test,n_neighbors=k)\n",
    "    \n",
    "    label_test_pred = np.zeros(len(indices_map))\n",
    "    \n",
    "    for num in range(len(indices_map)):\n",
    "        label_test_pred[num] = np.argmax(np.bincount(label_train[indices_map[num,:]]))\n",
    "    \n",
    "    return label_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_knn_tiny(label_classes, label_train_list, img_train_list, label_test_list, img_test_list):\n",
    "    # To do\n",
    "    #1\n",
    "    feature_vec = []\n",
    "    stride, size = (20,20)\n",
    "    k = 10\n",
    "    for name in img_train_list :\n",
    "        img = cv2.imread(name,0)\n",
    "        feature = get_tiny_image(img, output_size)\n",
    "        feature_vec.append( feature.reshape(np.prod(feature.shape)))\n",
    "    #2\n",
    "    feature_test_vec = []\n",
    "    for name in img_test_list :\n",
    "        img = cv2.imread(name,0)\n",
    "        feature_test = get_tiny_image(img, output_size)\n",
    "        feature_test_vec.append( feature_test.reshape(np.prod(feature_test.shape)))  \n",
    "    #3  \n",
    "    label_train_set = []\n",
    "    for item in label_train_list:\n",
    "        label_train_set.append(label_classes.index(item))\n",
    "    \n",
    "    label_train_set = np.array(label_train_set)\n",
    "    #4\n",
    "    label_test_set = []\n",
    "    for item in label_test_list:\n",
    "        label_test_set.append(label_classes.index(item))\n",
    "    \n",
    "    label_test_set = np.array(label_test_set)\n",
    "    \n",
    "    \n",
    "    #predict\n",
    "    label_test_pred = predict_knn(feature_vec, label_train_set, feature_test_vec, k)\n",
    "    \n",
    "    #confusion matrix]\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    y_true = label_test_set\n",
    "    y_pred = label_test_pred\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "    accuracy = np.trace(confusion)/np.sum(np.sum(confusion))\n",
    "    \n",
    "    \n",
    "    visualize_confusion_matrix(confusion, accuracy, label_classes)\n",
    "    return confusion, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_visual_dictionary(dense_feature_list, dic_size=50):\n",
    "    # To do\n",
    "    dense_feature=np.zeros((1,128))\n",
    "    \n",
    "    for item in dense_feature_list:\n",
    "\n",
    "        dense_feature = np.concatenate((dense_feature, item), axis=0)\n",
    "    \n",
    "    dense_feature_set = np.delete(dense_feature, 0, 0)\n",
    "    \n",
    "    kmeans = KMeans(n_clusters = dic_size,n_init=30,max_iter=100).fit(dense_feature_set)\n",
    "    vocab = kmeans.cluster_centers_\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_bow(feature, vocab):\n",
    "    # To do\n",
    "    nbrs = NearestNeighbors(algorithm='auto').fit(vocab) \n",
    "    \n",
    "    distances_map,indices_map =nbrs.kneighbors(feature,n_neighbors=1)\n",
    "    \n",
    "    bow_feature_pre = indices_map.reshape((1,len(indices_map)))\n",
    "    \n",
    "    #print(bow_feature_pre)\n",
    "    \n",
    "    bow_feature = np.bincount(bow_feature_pre[0],minlength=len(vocab))/len(feature)\n",
    "    \n",
    "    \n",
    "    return bow_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_knn_bow(label_classes, label_train_list, img_train_list, label_test_list, img_test_list):\n",
    "    # To do\n",
    "    stride, size = (16,16)\n",
    "    cluster_num = 70\n",
    "    \n",
    "    dense_feature_list=[]\n",
    "    for name in img_train_list :\n",
    "        img = cv2.imread(name,0)\n",
    "        dense_feature = compute_dsift(img, stride, size)\n",
    "        dense_feature_list.append(dense_feature)\n",
    "\n",
    "    vocab = build_visual_dictionary(dense_feature_list, cluster_num)\n",
    "    np.savetxt('test_knn_bow_out', vocab, delimiter=',')\n",
    "    \n",
    "    vocab_feature_list=np.zeros((1,cluster_num))\n",
    "    for name in img_train_list :\n",
    "        img = cv2.imread(name,0)\n",
    "        dense_feature = compute_dsift(img, stride, size)\n",
    "        \n",
    "        bow_feature = compute_bow(dense_feature, vocab)\n",
    "        \n",
    "        #print(bow_feature.shape)\n",
    "        \n",
    "        vocab_feature_list = np.concatenate((vocab_feature_list, np.array([bow_feature])), axis=0)\n",
    "        \n",
    "    vocab_feature_list = np.delete(vocab_feature_list, 0, 0)\n",
    "        \n",
    "        #vocab_feature_list.append(bow_feature)\n",
    "    \n",
    "    #2\n",
    "    vocab_feature_test_list = np.zeros((1,cluster_num))\n",
    "    for name in img_test_list :\n",
    "        img = cv2.imread(name,0)\n",
    "        dense_feature_test = compute_dsift(img, stride, size)\n",
    "        \n",
    "        bow_feature_test = compute_bow(dense_feature_test, vocab)\n",
    "        \n",
    "        #vocab_feature_test_list.append(dense_feature_test)\n",
    "        vocab_feature_test_list = np.concatenate((vocab_feature_test_list, np.array([bow_feature_test])), axis=0)\n",
    "        \n",
    "    vocab_feature_test_list = np.delete(vocab_feature_test_list, 0, 0)\n",
    "        \n",
    "    #3  \n",
    "    label_train_set = []\n",
    "    for item in label_train_list:\n",
    "        label_train_set.append(label_classes.index(item))\n",
    "    \n",
    "    label_train_set = np.array(label_train_set)\n",
    "    #4\n",
    "    label_test_set = []\n",
    "    for item in label_test_list:\n",
    "        label_test_set.append(label_classes.index(item))\n",
    "    \n",
    "    label_test_set = np.array(label_test_set)\n",
    "    \n",
    "    k = 10\n",
    "    #print(vocab_feature_list.shape,vocab_feature_test_list.shape)\n",
    "    \n",
    "    label_test_pred = predict_knn(vocab_feature_list, label_train_set, vocab_feature_test_list, k)\n",
    "    \n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    y_true = label_test_set\n",
    "    y_pred = label_test_pred\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "    accuracy = np.trace(confusion)/np.sum(np.sum(confusion))\n",
    "    \n",
    "    \n",
    "    visualize_confusion_matrix(confusion, accuracy, label_classes)\n",
    "\n",
    "    return confusion, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes: ['Kitchen', 'Store', 'Bedroom', 'LivingRoom', 'Office', 'Industrial', 'Suburb', 'InsideCity', 'TallBuilding', 'Street', 'Highway', 'OpenCountry', 'Coast', 'Mountain', 'Forest']\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'visualize_confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-c917b7bd6088>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlabel_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_train_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_train_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_test_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_test_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_dataset_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./scene_classification_data\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mclassify_knn_bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabel_classes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_train_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_train_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_test_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimg_test_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-8e75a1c0b050>\u001b[0m in \u001b[0;36mclassify_knn_bow\u001b[1;34m(label_classes, label_train_list, img_train_list, label_test_list, img_test_list)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m     \u001b[0mvisualize_confusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconfusion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_classes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mconfusion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'visualize_confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "label_classes, label_train_list, img_train_list, label_test_list, img_test_list = extract_dataset_info(\"./scene_classification_data\")\n",
    "    \n",
    "classify_knn_bow(label_classes, label_train_list, img_train_list, label_test_list, img_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_svm(feature_train, label_train, feature_test, n_classes):\n",
    "    # To do\n",
    "    label_test_pred_map = np.zeros((len(feature_train),n_classes))\n",
    "    for group in range(n_classes):\n",
    "        clf = LinearSVC(tol=1e-5,C=2.0) \n",
    "        \n",
    "        label_train_new =np.zeros(len(label_train))\n",
    "        index = 0 \n",
    "        for item in label_train:\n",
    "            if item == group:\n",
    "                label_train_new[index] = 1\n",
    "            else:\n",
    "                label_train_new[index] = 0\n",
    "            index = index + 1\n",
    "        \n",
    "        clf.fit(feature_train, label_train_new)  \n",
    "    \n",
    "        label_test_pred_map[:,group] = clf.decision_function(feature_test)[:,0]\n",
    "    \n",
    "    label_test_pred = np.argmax(label_test_pred_map, axis=1)\n",
    "    return label_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_svm_old(feature_train, label_train, feature_test, n_classes):\n",
    "    # To do\n",
    "    clf = LinearSVC(tol=1e-5,C=10.0) \n",
    "    clf.fit(feature_train, label_train)  \n",
    "    \n",
    "    label_test_pred = clf.predict(feature_test)\n",
    "    \n",
    "    \n",
    "    return label_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_svm_bow(label_classes, label_train_list, img_train_list, label_test_list, img_test_list):\n",
    "    # To do\n",
    "    \n",
    "    stride, size = (16,16)\n",
    "    cluster_num = 80\n",
    "    \n",
    "    dense_feature_list=[]\n",
    "    for name in img_train_list :\n",
    "        img = cv2.imread(name,0)\n",
    "        dense_feature = compute_dsift(img, stride, size)\n",
    "        dense_feature_list.append(dense_feature)\n",
    "\n",
    "    vocab = build_visual_dictionary(dense_feature_list, cluster_num)\n",
    "    np.savetxt('test_svm_bow_out', vocab, delimiter=',')\n",
    "    #vocab = np.loadtxt('test_svm_bow_out', delimiter=',')\n",
    "    \n",
    "    \n",
    "    vocab_feature_list=np.zeros((1,cluster_num))\n",
    "    for name in img_train_list :\n",
    "        img = cv2.imread(name,0)\n",
    "        dense_feature = compute_dsift(img, stride, size)\n",
    "        \n",
    "        bow_feature = compute_bow(dense_feature, vocab)\n",
    "        \n",
    "        #print(bow_feature.shape)\n",
    "        \n",
    "        vocab_feature_list = np.concatenate((vocab_feature_list, np.array([bow_feature])), axis=0)\n",
    "        \n",
    "    vocab_feature_list = np.delete(vocab_feature_list, 0, 0)\n",
    "        \n",
    "        #vocab_feature_list.append(bow_feature)\n",
    "    \n",
    "    #2\n",
    "    vocab_feature_test_list = np.zeros((1,cluster_num))\n",
    "    for name in img_test_list :\n",
    "        img = cv2.imread(name,0)\n",
    "        dense_feature_test = compute_dsift(img, stride, size)\n",
    "        \n",
    "        bow_feature_test = compute_bow(dense_feature_test, vocab)\n",
    "        \n",
    "        #vocab_feature_test_list.append(dense_feature_test)\n",
    "        vocab_feature_test_list = np.concatenate((vocab_feature_test_list, np.array([bow_feature_test])), axis=0)\n",
    "        \n",
    "    vocab_feature_test_list = np.delete(vocab_feature_test_list, 0, 0)\n",
    "        \n",
    "    #3  \n",
    "    label_train_set = []\n",
    "    for item in label_train_list:\n",
    "        label_train_set.append(label_classes.index(item))\n",
    "    \n",
    "    label_train_set = np.array(label_train_set)\n",
    "    #4\n",
    "    label_test_set = []\n",
    "    for item in label_test_list:\n",
    "        label_test_set.append(label_classes.index(item))\n",
    "    \n",
    "    label_test_set = np.array(label_test_set)\n",
    "    \n",
    "    k = 20\n",
    "    #print(vocab_feature_list.shape,vocab_feature_test_list.shape)\n",
    "    \n",
    "    label_test_pred = predict_svm_old(vocab_feature_list, label_train_set, vocab_feature_test_list,len(label_classes))\n",
    "    \n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    \n",
    "    y_true = label_test_set\n",
    "    y_pred = label_test_pred\n",
    "    confusion = confusion_matrix(y_true, y_pred)\n",
    "    accuracy = np.trace(confusion)/np.sum(np.sum(confusion))\n",
    "    \n",
    "    visualize_confusion_matrix(confusion, accuracy, label_classes)\n",
    "    return confusion, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_confusion_matrix(confusion, accuracy, label_classes):\n",
    "    plt.title(\"accuracy = {:.3f}\".format(accuracy))\n",
    "    plt.imshow(confusion)\n",
    "    ax, fig = plt.gca(), plt.gcf()\n",
    "    plt.xticks(np.arange(len(label_classes)), label_classes)\n",
    "    plt.yticks(np.arange(len(label_classes)), label_classes)\n",
    "    # set horizontal alignment mode (left, right or center) and rotation mode(anchor or default)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=-30, ha=\"center\", rotation_mode=\"default\")\n",
    "    # avoid top and bottom part of heatmap been cut\n",
    "    ax.set_xticks(np.arange(len(label_classes) + 1) - .5, minor=True)\n",
    "    ax.set_yticks(np.arange(len(label_classes) + 1) - .5, minor=True)\n",
    "    ax.tick_params(which=\"minor\", bottom=False, left=False)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_classes, label_train_list, img_train_list, label_test_list, img_test_list = extract_dataset_info(\"./scene_classification_data\")\n",
    "\n",
    "classify_svm_bow(label_classes, label_train_list, img_train_list, label_test_list, img_test_list)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf.decision_function(feature_test)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # To do: replace with your dataset path\n",
    "    label_classes, label_train_list, img_train_list, label_test_list, img_test_list = extract_dataset_info(\"./scene_classification_data\")\n",
    "    \n",
    "    classify_knn_tiny(label_classes, label_train_list, img_train_list, label_test_list, img_test_list)\n",
    "\n",
    "    classify_knn_bow(label_classes, label_train_list, img_train_list, label_test_list, img_test_list)\n",
    "    \n",
    "    classify_svm_bow(label_classes, label_train_list, img_train_list, label_test_list, img_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
